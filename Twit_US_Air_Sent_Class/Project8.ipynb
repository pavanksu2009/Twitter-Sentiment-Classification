{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GuUlC4Y5kLNF"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "W-bv0d3nkLNG"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWXDrhRhkLNG",
    "outputId": "cd84f716-a83a-46ea-9cad-fc5879d5d9d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in c:\\users\\pavanksu2009\\.virtualenvs\\twit_us_air_sent_class-4caezrq-\\lib\\site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\pavanksu2009\\.virtualenvs\\twit_us_air_sent_class-4caezrq-\\lib\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in c:\\users\\pavanksu2009\\.virtualenvs\\twit_us_air_sent_class-4caezrq-\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\pavanksu2009\\.virtualenvs\\twit_us_air_sent_class-4caezrq-\\lib\\site-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pavanksu2009\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pavanksu2009\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re, string, unicodedata\n",
    "import contractions                                     \n",
    "from bs4 import BeautifulSoup                           \n",
    "import nltk  \n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujhpLH3HkLNG"
   },
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "Cj-BImx7kLNG"
   },
   "outputs": [],
   "source": [
    "# Read the file\n",
    "data=pd.read_csv('Analytical_base_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "clrab5kSkLNH",
    "outputId": "2c6b037a-48e9-4ce0-968d-d2d319f4881c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>virginamerica what   dhepburn say</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>virginamerica plus you have add commercials ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>virginamerica i do not today   must mean i n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>virginamerica it be really aggressive to bla...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>virginamerica and it be a really big bad thi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  airline_sentiment\n",
       "0                virginamerica what   dhepburn say                    1\n",
       "1    virginamerica plus you have add commercials ...                  2\n",
       "2    virginamerica i do not today   must mean i n...                  1\n",
       "3    virginamerica it be really aggressive to bla...                  0\n",
       "4    virginamerica and it be a really big bad thi...                  0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print first five rows of dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oIw856wRkLNH",
    "outputId": "791d9a29-c4f4-46ad-c8a8-52ac916f407d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    9178\n",
       "1    3099\n",
       "2    2363\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the value count of airline_sentiment\n",
    "data.airline_sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9xFDlZvvbXZ"
   },
   "source": [
    "--> \"0\" represents \"negative\" sentiments\n",
    "\n",
    "--> \"1\" represents \"neutral\" sentiments\n",
    "\n",
    "--> \"2\" represents \"positive\" sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFwbPYeWmoHk"
   },
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gk1O-c9gYmnH",
    "outputId": "b6ae3cf6-a709-440b-f040-110a887d8d5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 1000)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Count vectorizer\n",
    "vectorizer = CountVectorizer(max_features=1000)                # Keep only 1000 features as number of features will increase the processing time.\n",
    "data_count = vectorizer.fit_transform(data['text'])\n",
    "\n",
    "data_count = data_count.toarray()                        # Convert the data features to array.\n",
    "data_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nqqyTSVJgbIS",
    "outputId": "7adf4278-6f7b-47e8-b4f8-fc81b344cdec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 1000)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TFIDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "data_tfidf = vectorizer.fit_transform(data['text'])\n",
    "\n",
    "data_tfidf = data_tfidf.toarray()                        # Convert the data features to array.\n",
    "data_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1Lk8u4MrKvm"
   },
   "source": [
    "## Use Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "EB0IxMgWoV6U"
   },
   "outputs": [],
   "source": [
    "# Get target column\n",
    "labels = data['airline_sentiment']\n",
    "\n",
    "# Split data into training and testing set for Count Vector\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_count, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lAYMMWpUrRnV",
    "outputId": "48280e58-12b3-4311-c076-8e3ebe1b43bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(n_estimators=10, n_jobs=4)\n",
      "0.7173497267759563\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators=10, n_jobs=4)\n",
    "forest = forest.fit(x_train, y_train)\n",
    "print(forest)\n",
    "\n",
    "print(np.mean(cross_val_score(forest, data_count, labels, cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CIkoC7CTsSmG",
    "outputId": "2c8276b4-3631-40ac-81e7-0c6fa2efef0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2628  135   51]\n",
      " [ 456  363   65]\n",
      " [ 249  112  333]]\n"
     ]
    }
   ],
   "source": [
    "# Predict the result\n",
    "result = forest.predict(x_test)\n",
    "\n",
    "# Print confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, result)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "sKr2UPjTsteq"
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing set for TFIDF Vector\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_tfidf, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sye_Yn7wstdd",
    "outputId": "ed8ea34f-9466-4595-8f42-0065014d18ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(n_estimators=10, n_jobs=4)\n",
      "0.7200136612021859\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators=10, n_jobs=4)\n",
    "forest = forest.fit(x_train, y_train)\n",
    "print(forest)\n",
    "\n",
    "print(np.mean(cross_val_score(forest, data_count, labels, cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['challa_forest.sav']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib \n",
    "joblib.dump(forest, 'challa_forest.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7518214936247724"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = joblib.load('challa_forest.sav')\n",
    "loaded_model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LsyejlGqtsbH",
    "outputId": "f0db90e9-1f68-4766-a177-512df8b6fe24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2642  120   52]\n",
      " [ 480  339   65]\n",
      " [ 279   94  321]]\n"
     ]
    }
   ],
   "source": [
    "# Predict the result\n",
    "result = forest.predict(x_test)\n",
    "\n",
    "# Print confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, result)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMPHFQ3exJZA"
   },
   "source": [
    "## Summary\n",
    "\n",
    "* We used the text of the tweet and the sentiment represented by the tweet which could be either positive, negative or neutral.\n",
    "* The objective was to build a classification model.\n",
    "* The text pre-processing was done by removing HTML tags, replacing contractions, removing numbers, special characters and punctuations. We also converted text to lower case and used lemmatization for tokenized words.\n",
    "* This pre-processed data was then converted to numbers using vectorization techniques; count vectorization and TFIDF vectorization to be used for random forest classifier.\n",
    "* The random forest classifier was used for predicting the results.\n",
    "* For count vectorization technique, we got a cross validation score of 71.7% whereas for TFIDF vectorization, we got 72% 10 fold cross validation score.\n",
    "* The performance of the model can be increased by using different classification models or neural networks besides changing the number of features for vectorization and also by using other pre-processing techniques like removing stop words etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Project8.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Twit_US_Air_Sent_Class-4CaEZrq-",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "328e7932fa71edf082a87215e16523be2345f74378b3a444abb596505ec61080"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
